{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4cc39f",
   "metadata": {},
   "source": [
    "<h1>SparkSession & HDFS</h1>\n",
    "    In this demo I'm going to use sample CSV file with data about bank loans.<p> \n",
    "    I will perform the creation of dataframe from CSV file.<p> \n",
    "    Then I will perform various operations on it. As a result I'm going to save the file on HDFS and then retrieve it.\n",
    "    To begin with I need to import few modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c482a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.sql.functions import lower, col,trim, udf,struct,isnan,when, lit, avg, sum\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType,ArrayType,Row\n",
    "import gc, time , re, os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np  \n",
    "epochNow = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ba77c",
   "metadata": {},
   "source": [
    "<h3> Session config </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb325d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace89a6f",
   "metadata": {},
   "source": [
    "<h4> Reading source raw CSV file from the URL </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfPandas = pd.read_csv('https://people.math.sc.edu/Burkardt/datasets/csv/bankloan2.csv')\n",
    "display(bank_loanDfPandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668dc083",
   "metadata": {},
   "source": [
    "<h2> Rework on column names. Elimination of unwanted characters with\n",
    "<blockquote>'\\s+([a-zA-Z_][a-zA-Z_0-9]*)\\s*' regex.</blockquote> <p><p> Turning big case column letters into small case. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90458d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark=spark.createDataFrame(bank_loanDfPandas)\n",
    "\n",
    "for each in bank_loanDfSpark.schema.names:\n",
    "    bank_loanDfSpark = bank_loanDfSpark.withColumnRenamed(each,  re.sub(r'\\s+([a-zA-Z_][a-zA-Z_0-9]*)\\s*','',each.replace(' ', '').lower()))\n",
    "for colname in bank_loanDfSpark.columns:\n",
    "    bank_loanDfSpark = bank_loanDfSpark.withColumn(colname, trim(col(colname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93451b3",
   "metadata": {},
   "source": [
    "<h2> CSV table loaded and prepared for further work as Spark DataFrame</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eca71e7a",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "| id|amount|salary|ratio|age|  occupation| property|type|outcome|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "|  1|245100| 66400| 3.69| 44|  industrial|     farm| stb|  repay|\n",
    "|  2| 90600| 75300|  1.2| 41|  industrial|     farm| stb|  repay|\n",
    "|  3|195600| 52100| 3.75| 37|  industrial|     farm| ftb|default|\n",
    "|  4|157800| 67600| 2.33| 44|  industrial|apartment| ftb|  repay|\n",
    "|  5|150800| 35800| 4.21| 39|professional|apartment| stb|default|\n",
    "|  6|133000| 45300| 2.94| 29|  industrial|     farm| ftb|default|\n",
    "|  7|193100| 73200| 2.64| 38|professional|    house| ftb|  repay|\n",
    "|  8|215000| 77600| 2.77| 17|professional|     farm| ftb|  repay|\n",
    "|  9| 83000| 62500| 1.33| 30|professional|    house| ftb|  repay|\n",
    "| 10|186100| 49200| 3.78| 30|  industrial|    house| ftb|default|\n",
    "| 11|161500| 53300| 3.03| 28|professional|apartment| stb|  repay|\n",
    "| 12|157400| 63900| 2.46| 30|professional|     farm| stb|  repay|\n",
    "| 13|210000| 54200| 3.87| 43|professional|apartment| ftb|  repay|\n",
    "| 14|209700| 53000| 3.96| 39|  industrial|     farm| ftb|default|\n",
    "| 15|143200| 65300| 2.19| 32|  industrial|apartment| ftb|default|\n",
    "| 16|203000| 64400| 3.15| 44|  industrial|     farm| ftb|  repay|\n",
    "| 17|247800| 63800| 3.88| 46|  industrial|    house| stb|  repay|\n",
    "| 18|162700| 77400|  2.1| 37|professional|    house| ftb|  repay|\n",
    "| 19|213300| 61100| 3.49| 21|  industrial|apartment| ftb|default|\n",
    "| 20|284100| 32300|  8.8| 51|  industrial|     farm| ftb|default|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2c510",
   "metadata": {},
   "source": [
    "<h2> Checking schema of a table </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ad605",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3419f865",
   "metadata": {},
   "source": [
    " |-- id: long (nullable = true)\n",
    " |-- amount: long (nullable = true)\n",
    " |-- salary: long (nullable = true)\n",
    " |-- ratio: double (nullable = true)\n",
    " |-- age: long (nullable = true)\n",
    " |-- occupation: string (nullable = true)\n",
    " |-- property: string (nullable = true)\n",
    " |-- type: string (nullable = true)\n",
    " |-- outcome: string (nullable = true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158bf612",
   "metadata": {},
   "source": [
    "<h2>Some sample filtering job. This time we are filtering all records with age equal to '44'.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark.filter(col('age') == '44').show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c36d618",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+----------+---------+----+-------+\n",
    "| id|amount|salary|ratio|age|occupation| property|type|outcome|\n",
    "+---+------+------+-----+---+----------+---------+----+-------+\n",
    "|  1|245100| 66400| 3.69| 44|industrial|     farm| stb|  repay|\n",
    "|  4|157800| 67600| 2.33| 44|industrial|apartment| ftb|  repay|\n",
    "| 16|203000| 64400| 3.15| 44|industrial|     farm| ftb|  repay|\n",
    "+---+------+------+-----+---+----------+---------+----+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78264bb2",
   "metadata": {},
   "source": [
    "<h2>Counting average salary with use of of 'avg' aggregation funciton. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a06874",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark.select(avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0789811f",
   "metadata": {},
   "source": [
    "+-----------+\n",
    "|avg(salary)|\n",
    "+-----------+\n",
    "|    59220.0|\n",
    "+-----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92404097",
   "metadata": {},
   "source": [
    "<h2> Saving the table on HDFS file system as a parquet table in overwrite mode.</h2>\n",
    "<h4> Of course in normal scenario we wouldn't allow such small file in target filesystem. Such saving operation exist only as a POC.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e50dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Dataframe into HDFS\n",
    "# Repartition it by \"Country\" column before storing as parquet files in Hadoop\n",
    "bank_loanDfSpark.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://hadoop-namenode:9000/salaries/{}_{}.parquet\".format('bank_loan',epochNow))\n",
    "print(\"Sales Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4174bc7",
   "metadata": {},
   "source": [
    "<h2> Loading the parquet table stored on HDFS file system</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from HDFS to confirm it was successfully stored\n",
    "df_load = spark.read.parquet(\"hdfs://hadoop-namenode:9000/salaries/{}_{}.parquet\".format('bank_loan',epochNow))\n",
    "print(\"Sales Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4b40d27",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "| id|amount|salary|ratio|age|  occupation| property|type|outcome|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "| 13|210000| 54200| 3.87| 43|professional|apartment| ftb|  repay|\n",
    "| 14|209700| 53000| 3.96| 39|  industrial|     farm| ftb|default|\n",
    "| 15|143200| 65300| 2.19| 32|  industrial|apartment| ftb|default|\n",
    "| 16|203000| 64400| 3.15| 44|  industrial|     farm| ftb|  repay|\n",
    "| 17|247800| 63800| 3.88| 46|  industrial|    house| stb|  repay|\n",
    "| 18|162700| 77400|  2.1| 37|professional|    house| ftb|  repay|\n",
    "| 19|213300| 61100| 3.49| 21|  industrial|apartment| ftb|default|\n",
    "| 20|284100| 32300|  8.8| 51|  industrial|     farm| ftb|default|\n",
    "| 21|154000| 48900| 3.15| 49|professional|    house| stb|  repay|\n",
    "| 22|112800| 79700| 1.42| 41|professional|    house| ftb|  repay|\n",
    "| 23|252000| 59700| 4.22| 27|professional|    house| stb|default|\n",
    "| 24|175200| 39900| 4.39| 37|professional|apartment| stb|default|\n",
    "| 25|149700| 58600| 2.55| 35|  industrial|     farm| stb|default|\n",
    "|  1|245100| 66400| 3.69| 44|  industrial|     farm| stb|  repay|\n",
    "|  2| 90600| 75300|  1.2| 41|  industrial|     farm| stb|  repay|\n",
    "|  3|195600| 52100| 3.75| 37|  industrial|     farm| ftb|default|\n",
    "|  4|157800| 67600| 2.33| 44|  industrial|apartment| ftb|  repay|\n",
    "|  5|150800| 35800| 4.21| 39|professional|apartment| stb|default|\n",
    "|  6|133000| 45300| 2.94| 29|  industrial|     farm| ftb|default|\n",
    "|  7|193100| 73200| 2.64| 38|professional|    house| ftb|  repay|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a9d1c",
   "metadata": {},
   "source": [
    "<h2> Summing up all loans from table. With help of 'sum' function. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load.select(sum(\"amount\").alias(\"all_loans_summed_up\")).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62ba2729",
   "metadata": {},
   "source": [
    "+-------------------+\n",
    "|all_loans_summed_up|\n",
    "+-------------------+\n",
    "|            4486500|\n",
    "+-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802f360",
   "metadata": {},
   "source": [
    "<h2> Now let's sort the records by the 'age' value in descending order</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8828e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_loanDfSpark.orderBy(col(\"age\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a78fcefb",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "|id |amount|salary|ratio|age|occupation  |property |type|outcome|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "|20 |284100|32300 |8.8  |51 |industrial  |farm     |ftb |default|\n",
    "|21 |154000|48900 |3.15 |49 |professional|house    |stb |repay  |\n",
    "|17 |247800|63800 |3.88 |46 |industrial  |house    |stb |repay  |\n",
    "|4  |157800|67600 |2.33 |44 |industrial  |apartment|ftb |repay  |\n",
    "|16 |203000|64400 |3.15 |44 |industrial  |farm     |ftb |repay  |\n",
    "|1  |245100|66400 |3.69 |44 |industrial  |farm     |stb |repay  |\n",
    "|13 |210000|54200 |3.87 |43 |professional|apartment|ftb |repay  |\n",
    "|22 |112800|79700 |1.42 |41 |professional|house    |ftb |repay  |\n",
    "|2  |90600 |75300 |1.2  |41 |industrial  |farm     |stb |repay  |\n",
    "|5  |150800|35800 |4.21 |39 |professional|apartment|stb |default|\n",
    "|14 |209700|53000 |3.96 |39 |industrial  |farm     |ftb |default|\n",
    "|7  |193100|73200 |2.64 |38 |professional|house    |ftb |repay  |\n",
    "|3  |195600|52100 |3.75 |37 |industrial  |farm     |ftb |default|\n",
    "|18 |162700|77400 |2.1  |37 |professional|house    |ftb |repay  |\n",
    "|24 |175200|39900 |4.39 |37 |professional|apartment|stb |default|\n",
    "|25 |149700|58600 |2.55 |35 |industrial  |farm     |stb |default|\n",
    "|15 |143200|65300 |2.19 |32 |industrial  |apartment|ftb |default|\n",
    "|12 |157400|63900 |2.46 |30 |professional|farm     |stb |repay  |\n",
    "|9  |83000 |62500 |1.33 |30 |professional|house    |ftb |repay  |\n",
    "|10 |186100|49200 |3.78 |30 |industrial  |house    |ftb |default|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5db9a",
   "metadata": {},
   "source": [
    "<h2>Let's do some quick example of an union operation. First we filter one dataframe with property column value equal to 'farm'. Then a second one this time with 'apartment' value. Then with help of union() we may get the desired result</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_property_farm = bank_loanDfSpark.filter(col('property') == 'farm')\n",
    "df_property_farm.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d795142",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+--------+----+-------+\n",
    "| id|amount|salary|ratio|age|  occupation|property|type|outcome|\n",
    "+---+------+------+-----+---+------------+--------+----+-------+\n",
    "|  1|245100| 66400| 3.69| 44|  industrial|    farm| stb|  repay|\n",
    "|  2| 90600| 75300|  1.2| 41|  industrial|    farm| stb|  repay|\n",
    "|  3|195600| 52100| 3.75| 37|  industrial|    farm| ftb|default|\n",
    "|  6|133000| 45300| 2.94| 29|  industrial|    farm| ftb|default|\n",
    "|  8|215000| 77600| 2.77| 17|professional|    farm| ftb|  repay|\n",
    "| 12|157400| 63900| 2.46| 30|professional|    farm| stb|  repay|\n",
    "| 14|209700| 53000| 3.96| 39|  industrial|    farm| ftb|default|\n",
    "| 16|203000| 64400| 3.15| 44|  industrial|    farm| ftb|  repay|\n",
    "| 20|284100| 32300|  8.8| 51|  industrial|    farm| ftb|default|\n",
    "| 25|149700| 58600| 2.55| 35|  industrial|    farm| stb|default|\n",
    "+---+------+------+-----+---+------------+--------+----+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_property_apartment= bank_loanDfSpark.filter(col('property') == 'apartment')\n",
    "df_property_apartment.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3457b9b1",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "| id|amount|salary|ratio|age|  occupation| property|type|outcome|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "|  4|157800| 67600| 2.33| 44|  industrial|apartment| ftb|  repay|\n",
    "|  5|150800| 35800| 4.21| 39|professional|apartment| stb|default|\n",
    "| 11|161500| 53300| 3.03| 28|professional|apartment| stb|  repay|\n",
    "| 13|210000| 54200| 3.87| 43|professional|apartment| ftb|  repay|\n",
    "| 15|143200| 65300| 2.19| 32|  industrial|apartment| ftb|default|\n",
    "| 19|213300| 61100| 3.49| 21|  industrial|apartment| ftb|default|\n",
    "| 24|175200| 39900| 4.39| 37|professional|apartment| stb|default|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = df_property_farm.union(df_property_apartment)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea4e2a76",
   "metadata": {},
   "source": [
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "| id|amount|salary|ratio|age|  occupation| property|type|outcome|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+\n",
    "|  1|245100| 66400| 3.69| 44|  industrial|     farm| stb|  repay|\n",
    "|  2| 90600| 75300|  1.2| 41|  industrial|     farm| stb|  repay|\n",
    "|  3|195600| 52100| 3.75| 37|  industrial|     farm| ftb|default|\n",
    "|  6|133000| 45300| 2.94| 29|  industrial|     farm| ftb|default|\n",
    "|  8|215000| 77600| 2.77| 17|professional|     farm| ftb|  repay|\n",
    "| 12|157400| 63900| 2.46| 30|professional|     farm| stb|  repay|\n",
    "| 14|209700| 53000| 3.96| 39|  industrial|     farm| ftb|default|\n",
    "| 16|203000| 64400| 3.15| 44|  industrial|     farm| ftb|  repay|\n",
    "| 20|284100| 32300|  8.8| 51|  industrial|     farm| ftb|default|\n",
    "| 25|149700| 58600| 2.55| 35|  industrial|     farm| stb|default|\n",
    "|  4|157800| 67600| 2.33| 44|  industrial|apartment| ftb|  repay|\n",
    "|  5|150800| 35800| 4.21| 39|professional|apartment| stb|default|\n",
    "| 11|161500| 53300| 3.03| 28|professional|apartment| stb|  repay|\n",
    "| 13|210000| 54200| 3.87| 43|professional|apartment| ftb|  repay|\n",
    "| 15|143200| 65300| 2.19| 32|  industrial|apartment| ftb|default|\n",
    "| 19|213300| 61100| 3.49| 21|  industrial|apartment| ftb|default|\n",
    "| 24|175200| 39900| 4.39| 37|professional|apartment| stb|default|\n",
    "+---+------+------+-----+---+------------+---------+----+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fff13f",
   "metadata": {},
   "source": [
    "<h2> Now let's proceed to present the result of join operation. First we need to select 1st dataframe. We select 'id, amount, ratio and age' columns </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c035351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_1 = bank_loanDfSpark.select(col(\"id\"), col(\"amount\"), col(\"ratio\"), col(\"age\"))\n",
    "df_select_1.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e30f337",
   "metadata": {},
   "source": [
    "+---+------+-----+---+\n",
    "| id|amount|ratio|age|\n",
    "+---+------+-----+---+\n",
    "|  1|245100| 3.69| 44|\n",
    "|  2| 90600|  1.2| 41|\n",
    "|  3|195600| 3.75| 37|\n",
    "|  4|157800| 2.33| 44|\n",
    "|  5|150800| 4.21| 39|\n",
    "|  6|133000| 2.94| 29|\n",
    "|  7|193100| 2.64| 38|\n",
    "|  8|215000| 2.77| 17|\n",
    "|  9| 83000| 1.33| 30|\n",
    "| 10|186100| 3.78| 30|\n",
    "| 11|161500| 3.03| 28|\n",
    "| 12|157400| 2.46| 30|\n",
    "| 13|210000| 3.87| 43|\n",
    "| 14|209700| 3.96| 39|\n",
    "| 15|143200| 2.19| 32|\n",
    "| 16|203000| 3.15| 44|\n",
    "| 17|247800| 3.88| 46|\n",
    "| 18|162700|  2.1| 37|\n",
    "| 19|213300| 3.49| 21|\n",
    "| 20|284100|  8.8| 51|\n",
    "+---+------+-----+---+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d517fb",
   "metadata": {},
   "source": [
    "<h2> Second we need to select 2nd dataframe. We select 'id, occupation, property and outcome' columns </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_2 = bank_loanDfSpark.select(col(\"id\"), col(\"occupation\"), col(\"property\"), col('outcome'))\n",
    "df_select_2.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70bc5e2a",
   "metadata": {},
   "source": [
    "+---+------------+---------+-------+\n",
    "| id|  occupation| property|outcome|\n",
    "+---+------------+---------+-------+\n",
    "|  1|  industrial|     farm|  repay|\n",
    "|  2|  industrial|     farm|  repay|\n",
    "|  3|  industrial|     farm|default|\n",
    "|  4|  industrial|apartment|  repay|\n",
    "|  5|professional|apartment|default|\n",
    "|  6|  industrial|     farm|default|\n",
    "|  7|professional|    house|  repay|\n",
    "|  8|professional|     farm|  repay|\n",
    "|  9|professional|    house|  repay|\n",
    "| 10|  industrial|    house|default|\n",
    "| 11|professional|apartment|  repay|\n",
    "| 12|professional|     farm|  repay|\n",
    "| 13|professional|apartment|  repay|\n",
    "| 14|  industrial|     farm|default|\n",
    "| 15|  industrial|apartment|default|\n",
    "| 16|  industrial|     farm|  repay|\n",
    "| 17|  industrial|    house|  repay|\n",
    "| 18|professional|    house|  repay|\n",
    "| 19|  industrial|apartment|default|\n",
    "| 20|  industrial|     farm|default|\n",
    "+---+------------+---------+-------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4a7ca",
   "metadata": {},
   "source": [
    "<h2> Now we may see the result of inner join operation on 'id' column. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_of_join_df = df_select_1.join(df_select_2, df_select_1.id ==  df_select_2.id, \"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fcdbef1",
   "metadata": {},
   "source": [
    "+---+------+-----+---+---+------------+---------+-------+\n",
    "|id |amount|ratio|age|id |occupation  |property |outcome|\n",
    "+---+------+-----+---+---+------------+---------+-------+\n",
    "|7  |193100|2.64 |38 |7  |professional|house    |repay  |\n",
    "|15 |143200|2.19 |32 |15 |industrial  |apartment|default|\n",
    "|11 |161500|3.03 |28 |11 |professional|apartment|repay  |\n",
    "|3  |195600|3.75 |37 |3  |industrial  |farm     |default|\n",
    "|8  |215000|2.77 |17 |8  |professional|farm     |repay  |\n",
    "|22 |112800|1.42 |41 |22 |professional|house    |repay  |\n",
    "|16 |203000|3.15 |44 |16 |industrial  |farm     |repay  |\n",
    "|5  |150800|4.21 |39 |5  |professional|apartment|default|\n",
    "|18 |162700|2.1  |37 |18 |professional|house    |repay  |\n",
    "|17 |247800|3.88 |46 |17 |industrial  |house    |repay  |\n",
    "|6  |133000|2.94 |29 |6  |industrial  |farm     |default|\n",
    "|19 |213300|3.49 |21 |19 |industrial  |apartment|default|\n",
    "|23 |252000|4.22 |27 |23 |professional|house    |default|\n",
    "|25 |149700|2.55 |35 |25 |industrial  |farm     |default|\n",
    "|24 |175200|4.39 |37 |24 |professional|apartment|default|\n",
    "|9  |83000 |1.33 |30 |9  |professional|house    |repay  |\n",
    "|1  |245100|3.69 |44 |1  |industrial  |farm     |repay  |\n",
    "|20 |284100|8.8  |51 |20 |industrial  |farm     |default|\n",
    "|10 |186100|3.78 |30 |10 |industrial  |house    |default|\n",
    "|4  |157800|2.33 |44 |4  |industrial  |apartment|repay  |\n",
    "+---+------+-----+---+---+------------+---------+-------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
